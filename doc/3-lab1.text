Lab 1 {#lab-1 .unnumbered}
=====

In this lab, we will design and develop the code in Spark to process GDelt
data, which will be used in lab 2 to scale the analysis to the entire dataset.
We will first give a brief introduction to the various technologies used in
this lab.

## Assignment

Now that we have introduced the different technologies, we can start talking
about the goal of the first lab. In this lab you will write the application in
Spark that analyzes GDelt and constructs the 10 most talked about topics per
day. For the first lab you will write the prototype that you check on your
local machine.

We will use the GDelt version 2 GKG files. The format these files are in is tab
separated values. The exact specification of each columns and details can be
found in the [GKG codebook]. The schema of the files can be read in
`headers.csv` in the `data` folder. The columns that are most relevant are the
"date" column and the "allNames" column.

In the `data` folder you will also find a script called `get_data`. This script
will download a number of sample files to your computer, and generate a
document with the paths to all these files.

```{.sh}
$./get_data 4
...
wget downloading
...

$cat local_index.txt
/path/to/this/repo/SBD-2018/data/segment/20150218230000.gkg.csv
/path/to/this/repo/SBD-2018/data/segment/20150218231500.gkg.csv
/path/to/this/repo/SBD-2018/data/segment/20150218233000.gkg.csv
/path/to/this/repo/SBD-2018/data/segment/20150218234500.gkg.csv
```

The script will put all downloaded files in the `segment` folder. `wget`
timestamps the downloads, so it will not update the files when you want to
generate a local index for 20 files if you had 10 before.

You can use these local files for the first lab assignment to test your
application, and build some understanding of the scaling behaviour on a single
machine.

An example output of this system based on 10 segments would be:

```
DateResult(2015-02-19,List((United States,1497), (Islamic State,1233), (New
York,1058), (United Kingdom,735), (White House,723), (Los Angeles,620), (New
Zealand,590), (Associated Press,498), (San Francisco,479), (Practice Wrestling
Room,420)))
DateResult(2015-02-18,List((Islamic State,1787), (United States,1210), (New
York,727), (White House,489), (Los Angeles,424), (Associated Press,385), (New
Zealand,353), (United Kingdom,325), (Jeb Bush,298), (Practice Wrestling
Room,280)))
```

Or in JSON:

```{.json}
{"data":"2015-02-19","result":[{"topic":"United
States","count":1497},{"topic":"Islamic State","count":1233},{"topic":"New
York","count":1058},{"topic":"United Kingdom","count":735},{"topic":"White
House","count":723},{"topic":"Los Angeles","count":620},{"topic":"New
Zealand","count":590},{"topic":"Associated Press","count":498},{"topic":"San
Francisco","count":479},{"topic":"Practice Wrestling Room","count":420}]}
{"data":"2015-02-18","result":[{"topic":"Islamic
State","count":1787},{"topic":"United States","count":1210},{"topic":"New
York","count":727},{"topic":"White House","count":489},{"topic":"Los
Angeles","count":424},{"topic":"Associated Press","count":385},{"topic":"New
Zealand","count":353},{"topic":"United Kingdom","count":325},{"topic":"Jeb
Bush","count":298},{"topic":"Practice Wrestling Room","count":280}]}
```

The exact counts can vary depending on how you count, for instance if a name
is mentioned multiple times per article, do you count it once or multiple
times? Something in between? Do you filter out some names that are false
positives ("ParentCategory" seems to be a particular common one)? You are free
to implement it whatever you think is best, and are encouraged to experiment
with this. Document your choices in your report.

## Deliverables

The deliverables for the first lab are:

1.  A Dataframe/Dataset-based implementation of the GDelt analysis,
2.  An RDD-based implementation of the GDelt analysis,
3.  A report containing:
    1.  Outline of your implementation and approach (½–1 page);
    2.  Answers to the questions listed below. Be as concise as you can!

The deadline of this lab will be announced on Brightspace. Your report and code
will be discussed in a brief oral examination during the lab, the schedule of
which will become available later.


## Questions

General questions:

1.  In typical use, what kind of operation would be more expensive, a narrow
    dependency or a wide dependency? Why? (max. 50 words)
2.  What is the shuffle operation and why is it such an important topic in
    Spark optimization? (max. 100 words)
3.  In what way can Dataframes and Datasets improve performance both in
    compute, but also in the distributing of data compared to RDDs? Under what
    conditions will Dataframes perform better than RDDs? (max. 100 words)
4.  Consider the following scenario. You are running a Spark program on a big
    data cluster with 10 worker nodes and a single master node. One of the
    worker nodes fails. In what way does Spark's programming model help you
    recover the lost work? (Think about the execution plan or DAG) (max. 50
    words)
5.  We might distinguish the following five conceptual parallel programming
    models:

    1.  farmer/worker
    2.  divide and conquer
    3.  data parallelism
    4.  function parallelism
    5.  bulk-synchronous

    Pick one of these models and explain why it does or does not fit Spark's
    programming model. (max. 100 words)

Implementation analysis questions:

1.  Measure the execution times for 10, 20, 50, 100 and 150 segments. Do you
    observe a difference in execution time between your Dataframe and RDD
    implementations? Is this what you expect and why? (max. 50 words)
2.  How will your application scale when increasing the amount of analyzed
    segments? What do you expect the progression in execution time will be for, 100,
    1000, 10000 segments? (max. 50 words)
3.  If you extrapolate the scaling behavior on your machine, using your results
    from question 1, to the entire dataset, how much time will it take to
    process the entire dataset? Is this extrapolation reasonable for a single
    machine? (max. 50 words)
4.  Now suppose you had a cluster of identical machines with that you performed the
    analysis on. How many machines do you think you would need to process the
    entire dataset in under an hour? Do you think this is a valid
    extrapolation? (max. 50 words)
5.  Suppose you would run this analysis for a company. What do you think would
    be an appropriate way to measure the performance? Would it be the time it
    takes to execute? The amount of money it takes to perform the analysis on
    the cluster? A combination of these two, or something else? Pick something
    you think would be an interesting metric, as this is the metric you will be
    optimizing in the 2nd lab! (max. 100 words)

  [GitHub Student Developer Pack]: https://education.github.com/pack
  [Spark overview page]: https://spark.apache.org/docs/latest/
  [lab's GitHub repository]: https://github.com/Tclv/SBD-Lab2/blob/master/get_dependencies.sh
  [Standard Big Data pipeline.]: ./images/big_data_stages
  [AWS CLI]: http://docs.aws.amazon.com/cli/latest/userguide/installing.html
  [AWS-CLI docs]: https://aws.amazon.com/cli/
  [Reddit AMA]: https://www.reddit.com/r/IAmA/comments/31bkue/im_matei_zaharia_creator_of_spark_and_cto_at/
  [AWS site]: https://aws.amazon.com/premiumsupport/knowledge-center/emr-pyspark-python-3x/
  [AWS website]: http://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html
  [AWS documentation]: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-limits.html
  [Scala language site]: https://docs.scala-lang.org/tour/tour-of-scala.html
  [lazy evaluation]: https://en.wikipedia.org/wiki/Lazy_evaluation
  [Spark SQL, DataFrames and Datasets Guide]: https://spark.apache.org/docs/latest/sql-programming-guide.html#overview
  [Spark SQL types]: https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/sql/types/package-frame.html
  [Java date format]: https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html
  [case class]: https://docs.scala-lang.org/tour/case-classes.html
  [RDDs]: https://spark.apache.org/docs/latest/rdd-programming-guide.html
  [Dataframes/Datasets]: https://spark.apache.org/docs/latest/sql-programming-guide.html
  [Spark documentation]: https://spark.apache.org/docs/2.3.1/api/scala/index.html#package
  [GKG codebook]: http://data.gdeltproject.org/documentation/GDELT-Global_Knowledge_Graph_Codebook-V2.1.pdf
  [log levels to warn]: https://stackoverflow.com/questions/27781187/how-to-stop-info-messages-displaying-on-spark-console
  [SBT build tool]: https://www.scala-sbt.org
